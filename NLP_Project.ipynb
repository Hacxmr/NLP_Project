{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTvGEf/XCCL6EdOf6XQO1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hacxmr/NLP_Project/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Summarization with Bias Detection in News"
      ],
      "metadata": {
        "id": "uHa5h2qlP1A1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tzXNNcxlPeZp",
        "outputId": "40f57401-b5e6-4625-ba53-ff091c84cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas tqdm selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text = \"Instead of promised 'acche din', days of debt have arrived: Congress's dig at Modi Government\"\n",
        "summary = summarizer(text, max_length=20, min_length=5, do_sample=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPEOH6dHPv2R",
        "outputId": "5e2738d7-5578-4120-c7e9-105862cbff65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mmwukqmQBjh",
        "outputId": "435f529a-8672-4da5-942d-97206803ee7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"Instead of promised 'acche din', days of debt have arrived: Congress's dig\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(text, max_length=30, min_length=5, do_sample=False)\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aumBLiwHTx5h",
        "outputId": "1ab4a1cc-dd16-477e-a172-dbb581c372b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 30, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instead of promised 'acche din', days of debt have arrived: Congress's dig at Modi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis"
      ],
      "metadata": {
        "id": "MyZ2vHlRUVq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "sentiment = sentiment_pipeline(summary[0]['summary_text'])\n",
        "print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbkUVAvmT1Mq",
        "outputId": "2805c31c-b806-4cb3-f9bc-6a45aee86550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9930257797241211}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Explore BART/T5 Models"
      ],
      "metadata": {
        "id": "rVEv40PbUxId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-base\",tokenizer=\"t5-base\")\n",
        "\n",
        "\n",
        "input_text = \"summarize: \"+text\n",
        "summary = summarizer(input_text, max_length=20, min_length=5, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-imv1izNUwUq",
        "outputId": "3b7f5057-3330-4501-c48a-5a3916d11504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instead of promised 'acche din', days of debt have arrived .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection - Scraping"
      ],
      "metadata": {
        "id": "P_Ro8XH6rwvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODFDfn60shs2",
        "outputId": "39e4e638-b3ed-438e-f60f-2cc11dd26d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium pandas transformers beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs-b5eWrzXLE",
        "outputId": "0cff8787-b42e-401e-d9aa-ee9d467444d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import tempfile\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")  # fix for profile conflict\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n"
      ],
      "metadata": {
        "id": "x4X8fRB4z2kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from urllib.parse import urljoin\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP Pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "bias_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://www.indiatoday.in/elections\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "KEYWORDS = [\n",
        "    \"delhi election\", \"delhi polls\", \"aap\", \"kejriwal\", \"delhi mla\",\n",
        "    \"bihar election\", \"bihar polls\", \"tejashwi\", \"nitish\", \"bjp\", \"rjd\", \"jdu\",\n",
        "    \"lok sabha\", \"2024\", \"campaign\", \"manifesto\", \"seat sharing\", \"voting\"\n",
        "]\n",
        "MAX_ARTICLES = 15\n",
        "\n",
        "def is_delhi_election(text):\n",
        "    return any(k in text.lower() for k in KEYWORDS)\n",
        "\n",
        "def get_article_links():\n",
        "    r = requests.get(BASE_URL, headers=HEADERS)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    links = []\n",
        "    for a in soup.find_all(\"a\", href=True, title=True):\n",
        "        href = urljoin(BASE_URL, a[\"href\"])\n",
        "        title = a[\"title\"].strip()\n",
        "        if is_delhi_election(title):\n",
        "            links.append((href, title))\n",
        "    return list(set(links))\n",
        "\n",
        "def extract_article_content(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 30)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def summarize_sentiment_bias(text):\n",
        "    summary = summarizer(text[:1024])[0]['summary_text']\n",
        "    sentiment = sentiment_analyzer(summary)[0]\n",
        "    bias = bias_classifier(summary, candidate_labels=[\"pro-government\", \"anti-government\", \"neutral\"])\n",
        "    return summary, sentiment[\"label\"], sentiment[\"score\"], bias[\"labels\"][0], bias[\"scores\"][0]\n",
        "\n",
        "def main():\n",
        "    articles = get_article_links()\n",
        "    print(f\"[+] Found {len(articles)} Delhi election-related articles\")\n",
        "\n",
        "    data = []\n",
        "    for i, (url, title) in enumerate(articles[:MAX_ARTICLES]):\n",
        "        print(f\"[{i+1}] Scraping: {title}\")\n",
        "        content = extract_article_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, senti_score, bias, bias_score = summarize_sentiment_bias(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"content\": content,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sentiment_score\": round(senti_score, 2),\n",
        "                \"bias\": bias,\n",
        "                \"bias_score\": round(bias_score, 2),\n",
        "                \"source\": \"India Today\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1, 2.5))\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"indiatoday_elections.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(df)} articles to indiatoday_delhi_elections.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aij83w9k9Z4D",
        "outputId": "e39eddfc-c2dc-4a98-8b0c-fcf684a3af65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Found 37 Delhi election-related articles\n",
            "[1] Scraping: BJP's Mohan Singh Bisht proposes to rename Mustafabad to Shivpuri\n",
            "[2] Scraping: Chose wrong path, focused on liquor: Anna Hazare on Arvind Kejriwal's poll loss\n",
            "[3] Scraping: Amanatullah Khan slams Rahul Gandhi for campaigning in Okhla to 'defeat' AAP\n",
            "[4] Scraping: BJP top brass holds hectic parleys to finalise Delhi's next Chief Minister\n",
            "[5] Scraping: Down but not out: Is Nitish Kumar still the X-factor in Bihar Politics?\n",
            "[6] Scraping: Nitish Kumar changes party for kursi: Congress targets BJP-JDU alliance\n",
            "[7] Scraping: BJP seeks dominance in Delhi civic body following Assembly success\n",
            "[8] Scraping: RJD leader pitches Tejashwi Yadav as chief ministerial face ahead of meet today\n",
            "[9] Scraping: Is Kanhaiya Kumar stealing Tejashwi Yadav's thunder on jobs, migration in Bihar?\n",
            "[10] Scraping: BJP considers candidates for Delhi Chief Minister's post after win\n",
            "[11] Scraping: BJP to release multiple CAG reports in upcoming Delhi assembly session\n",
            "[12] Scraping: Chirag Paswan's Bihar bid: Pressure tactic on BJP or strategy to replace Nitish?\n",
            "[13] Scraping: LJP (RV) projects Chirag Paswan as Chief Minister face, challenging Nitish Kumar\n",
            "[14] Scraping: Delhi BJP chief seeks to meet Lt Governor amid speculations over chief minister\n",
            "[15] Scraping: Masterstroke or damage-control? BJP's caste census sets ball rolling in Bihar\n",
            "[✓] Saved 15 articles to indiatoday_delhi_elections.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NDTV - SCRAPING AND ANALYSIS"
      ],
      "metadata": {
        "id": "iALI9BDjJsfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import tempfile\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP Pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "bias_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Focus keywords for Delhi and Bihar elections\n",
        "KEYWORDS = [\n",
        "    \"delhi election\", \"delhi polls\", \"aap\", \"kejriwal\", \"delhi mla\",\n",
        "    \"bihar election\", \"bihar polls\", \"tejashwi\", \"nitish\", \"bjp\", \"rjd\", \"jdu\",\n",
        "    \"lok sabha\", \"2024\", \"campaign\", \"manifesto\", \"seat sharing\", \"voting\"\n",
        "]\n",
        "\n",
        "# Selenium Setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "def is_target_region(text):\n",
        "    return any(keyword in text.lower() for keyword in KEYWORDS)\n",
        "\n",
        "def get_articles():\n",
        "    url = \"https://www.ndtv.com/elections\"\n",
        "    print(f\"[+] Visiting: {url}\")\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "    links = []\n",
        "\n",
        "    for tag in soup.find_all(\"a\", class_=\"crd_lnk\", href=True):\n",
        "        title = tag.get_text(strip=True)\n",
        "        href = tag[\"href\"]\n",
        "        if title and is_target_region(title):\n",
        "            links.append((title, href))\n",
        "\n",
        "    return list(set(links))\n",
        "\n",
        "def extract_article_content(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 30)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def analyze_text(text):\n",
        "    try:\n",
        "        summary = summarizer(text[:1024])[0][\"summary_text\"]\n",
        "        sentiment_result = sentiment_analyzer(summary)[0]\n",
        "        sentiment = sentiment_result[\"label\"]\n",
        "        sentiment_score = round(sentiment_result[\"score\"], 2)\n",
        "\n",
        "        bias_result = bias_classifier(summary, candidate_labels=[\"pro-government\", \"anti-government\", \"neutral\"])\n",
        "        bias = bias_result[\"labels\"][0]\n",
        "        bias_score = round(bias_result[\"scores\"][0], 2)\n",
        "\n",
        "        return summary, sentiment, sentiment_score, bias, bias_score\n",
        "    except Exception as e:\n",
        "        print(f\"[-] NLP error: {e}\")\n",
        "        return \"Summary failed\", \"UNKNOWN\", 0.0, \"UNKNOWN\", 0.0\n",
        "\n",
        "def main():\n",
        "    articles = get_articles()\n",
        "    print(f\"[+] Found {len(articles)} Delhi/Bihar election-related articles\")\n",
        "\n",
        "    data = []\n",
        "    for i, (title, url) in enumerate(articles[:15]):\n",
        "        print(f\"[{i+1}] Scraping: {title}\")\n",
        "        content = extract_article_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, senti_score, bias, bias_score = analyze_text(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"content\": content,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sentiment_score\": senti_score,\n",
        "                \"bias\": bias,\n",
        "                \"bias_score\": bias_score,\n",
        "                \"source\": \"NDTV\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"ndtv_delhi_bihar_election.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(df)} articles to ndtv_delhi_bihar_election.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz2dM9Jz-Tqd",
        "outputId": "4f85becd-ca73-492a-c7e9-0f009e408377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Visiting: https://www.ndtv.com/elections\n",
            "[+] Found 15 Delhi/Bihar election-related articles\n",
            "[1] Scraping: \"Governance Is Not Theatrics\": PM's Dig At AAP After BJP Wins Delhi\n",
            "[2] Scraping: Delhi Oath Ceremony On Thursday, BJP Yet To Name Chief Minister\n",
            "[3] Scraping: \"AAP Was Finished When...\": Ex Leader Ashutosh's Post After Delhi Drubbing\n",
            "[4] Scraping: Video: AAP's Office Doors Shut As Arvind Kejriwal, Manish Sisodia Lose Delhi\n",
            "[5] Scraping: \"Was Doing Good Job, But...\": Anna Hazare On Arvind Kejriwal's Poll Defeat\n",
            "[6] Scraping: \"2026MeinBengalKi Baari\": Mamata Banerjee Warned After BJP's Delhi Win\n",
            "[7] Scraping: AAP's Saurabh Bharadwaj Turns YouTuber After Delhi Election Loss\n",
            "[8] Scraping: \"Rule Of Lies Has Ended\": Amit Shah On BJP's Delhi Victory\n",
            "[9] Scraping: AAP To Replace Chief Minister In Punjab? What Bhagwant Mann Said\n",
            "[10] Scraping: How AAP-Congress Feud Split Opposition Votes\n",
            "[11] Scraping: AAP vs Congress In Punjab Over '30 MLAs In Touch With Us' Claim\n",
            "[12] Scraping: Congress-AAP's Unity Strongly Needed, Should Have Fought Delhi Polls Together: Amartya Sen\n",
            "[13] Scraping: Delhi Election Results 2025: BJP's Comeback, AAP's Loss Spark A Meme Fest\n",
            "[14] Scraping: \"Give Him Some Water\": PM Modi Pauses Speech To Check On Unwell BJP Worker\n",
            "[15] Scraping: PM Modi's Big Mention Of Yamuna In Delhi Victory Speech To BJP Workers\n",
            "[✓] Saved 15 articles to ndtv_delhi_bihar_election.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import tempfile\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load NLP models\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def classify_bias(text):\n",
        "    text = text.lower()\n",
        "    pro_gov = any(word in text for word in [\"modi\", \"bjp\", \"nda\", \"development\"])\n",
        "    anti_gov = any(word in text for word in [\"scam\", \"failure\", \"protest\", \"resign\", \"criticism\", \"controversy\"])\n",
        "    if pro_gov and not anti_gov:\n",
        "        return \"PRO-GOVERNMENT\"\n",
        "    elif anti_gov and not pro_gov:\n",
        "        return \"ANTI-GOVERNMENT\"\n",
        "    elif pro_gov and anti_gov:\n",
        "        return \"MIXED\"\n",
        "    return \"NEUTRAL\"\n",
        "\n",
        "# Keywords\n",
        "KEYWORDS = [\"delhi election\", \"bihar election\", \"2024\", \"lok sabha\", \"nda\", \"india bloc\", \"upa\", \"bjp\", \"congress\"]\n",
        "\n",
        "# Selenium setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# Fetch TOI Delhi and Bihar election articles\n",
        "def get_toi_articles():\n",
        "    urls = [\n",
        "        \"https://timesofindia.indiatimes.com/elections/assembly-elections/delhi\",\n",
        "        \"https://timesofindia.indiatimes.com/elections/assembly-elections/bihar\"\n",
        "    ]\n",
        "    articles = []\n",
        "    for url in urls:\n",
        "        driver.get(url)\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        tags = soup.find_all(\"a\", class_=\"SgKCA\", href=True)\n",
        "        for tag in tags:\n",
        "            title = tag.get_text(strip=True)\n",
        "            link = tag['href']\n",
        "            if any(k in title.lower() for k in KEYWORDS):\n",
        "                if not link.startswith(\"http\"):\n",
        "                    link = \"https://timesofindia.indiatimes.com\" + link\n",
        "                articles.append((title, link))\n",
        "            if len(articles) >= 15:\n",
        "                return articles\n",
        "    return articles\n",
        "\n",
        "# Extract article content\n",
        "def extract_article(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"div\", class_=\"Normal\")\n",
        "        if not paragraphs:\n",
        "            paragraphs = soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 30)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Failed to extract {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Main\n",
        "def main():\n",
        "    print(\"[+] Scraping Times of India for Delhi & Bihar election news\")\n",
        "    articles = get_toi_articles()\n",
        "    print(f\"[+] Found {len(articles)} relevant articles\")\n",
        "\n",
        "    records = []\n",
        "    for i, (title, url) in enumerate(articles):\n",
        "        print(f\"[{i+1}] {title}\")\n",
        "        content = extract_article(url)\n",
        "        if content:\n",
        "            summary = summarizer(content[:1024])[0]['summary_text']\n",
        "            sentiment_result = sentiment_analyzer(summary)[0]\n",
        "            sentiment = sentiment_result['label']\n",
        "            score = round(sentiment_result['score'], 2)\n",
        "            bias = classify_bias(summary)\n",
        "            records.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"confidence\": score,\n",
        "                \"bias\": bias,\n",
        "                \"content\": content,\n",
        "                \"source\": \"TOI\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(\"toi_delhi_bihar_elections.csv\", index=False)\n",
        "    print(\"[✓] Data saved to toi_delhi_bihar_elections.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGaNDakI-Sw",
        "outputId": "f61ed8df-6055-4342-e30e-598a4571cabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scraping Times of India for Delhi & Bihar election news\n",
            "[+] Found 15 relevant articles\n",
            "[1] All NCR states now have BJP governments, will give infra a big push, says PM Modi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 106. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2] Will work as constructive opposition: AAP; BJP thanks PM\n",
            "[3] Delhi elections: Four reserved seats shed reservations\n",
            "[4] In BJP's Delhi win, a booster for Nayab Singh Saini, Manohar Lal Khattar\n",
            "[5] House of controversy: Delhi BJP says CM won't occupy 'Sheesh Mahal'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 106. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6] Sirsa win weakens SAD in Delhi, BJP gains ground among Sikh voters\n",
            "[7] Delhi elections: Debutants taste nail-biters and commanding leads\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8] Delhi elections: In a rare four-cornered fight, Amanatullah on top in Okhla\n",
            "[9] Delhi election results: 'Saffrun' continues in Rohini as Vijender Gupta secures facile hat-trick\n",
            "[10] Bihar: Opposition boycotts swearing-in, says it’s betrayal of mandate\n",
            "[11] Bihar: LJP chief congratulates Nitish Kumar, BJP\n",
            "[12] Bihar: Congress flays Shivanand Tiwari, RJD distances from his statement\n",
            "[13] BJP gets 2 deputy CMs as diminished Nitish takes oath\n",
            "[14] Nitish Kumar BJP-nominated CM, tired and politically 'belittled': Prashant Kishor\n",
            "[15] NDA family will work together for progress of Bihar: PM Modi\n",
            "[✓] Data saved to toi_delhi_bihar_elections.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium pandas beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkVdt0tqGm1c",
        "outputId": "4016c22e-59bb-43ff-a127-3eb04543b7f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Bias detection (basic rule-based)\n",
        "def detect_bias(text):\n",
        "    t = text.lower()\n",
        "    if any(k in t for k in [\"bjp\", \"modi\", \"development\"]):\n",
        "        return \"Pro-Government\"\n",
        "    if any(k in t for k in [\"aam aadmi\", \"congress\", \"rjd\", \"opposition\"]):\n",
        "        return \"Anti-Government\"\n",
        "    return \"Neutral\"\n",
        "\n",
        "# Keywords for relevance\n",
        "KEYWORDS = [\"delhi\", \"bihar\", \"election\", \"assembly\", \"mla\", \"poll\", \"campaign\"]\n",
        "\n",
        "# Multiple sources from The Hindu\n",
        "START_URLS = [\n",
        "    \"https://www.thehindu.com/elections/\",\n",
        "    \"https://www.thehindu.com/elections/delhi-assembly/\",\n",
        "    \"https://www.thehindu.com/elections/bihar-assembly/\"\n",
        "]\n",
        "\n",
        "def is_relevant(text):\n",
        "    return any(k in text.lower() for k in KEYWORDS)\n",
        "\n",
        "def fetch_article_links():\n",
        "    all_links = []\n",
        "    seen = set()\n",
        "    for url in START_URLS:\n",
        "        print(f\"[+] Scanning: {url}\")\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        for div in soup.select(\"div.right-content\"):\n",
        "            h3 = div.find(\"h3\", class_=\"title\")\n",
        "            if h3 and h3.a:\n",
        "                title = h3.a.get_text(strip=True)\n",
        "                href = h3.a[\"href\"]\n",
        "                if href not in seen and is_relevant(title):\n",
        "                    all_links.append((title, href))\n",
        "                    seen.add(href)\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            title = a.get_text(strip=True)\n",
        "            href = a[\"href\"]\n",
        "            if href.startswith(\"https://www.thehindu.com\") and href not in seen and is_relevant(title):\n",
        "                all_links.append((title, href))\n",
        "                seen.add(href)\n",
        "    return all_links\n",
        "\n",
        "def extract_content(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paras = soup.select(\"div.ece-1705723324092 p\") or soup.find_all(\"p\")\n",
        "        return \" \".join(p.get_text(strip=True) for p in paras if len(p.get_text(strip=True)) > 30)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def analyze(text):\n",
        "    try:\n",
        "        summary = summarizer(text[:1024])[0]['summary_text']\n",
        "        sent = sentiment_analyzer(summary)[0]\n",
        "        bias = detect_bias(summary)\n",
        "        return summary, sent['label'], round(sent['score'], 2), bias\n",
        "    except:\n",
        "        return \"Summary failed\", \"UNKNOWN\", 0.0, \"Neutral\"\n",
        "\n",
        "def main():\n",
        "    articles = fetch_article_links()\n",
        "    print(f\"[+] Found {len(articles)} total relevant articles from The Hindu\")\n",
        "    data = []\n",
        "    for title, url in articles[:15]:\n",
        "        print(f\"🗂️  {title}\")\n",
        "        content = extract_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, sent_score, bias = analyze(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sent_score\": sent_score,\n",
        "                \"bias\": bias,\n",
        "                \"source\": \"The Hindu\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"thehindu_elections_delhi_bihar.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(data)} articles to thehindu_elections_delhi_bihar.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILIza-HGG84c",
        "outputId": "8b3d4b06-26d5-41f3-cc76-25951e79afca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scanning: https://www.thehindu.com/elections/\n",
            "[+] Scanning: https://www.thehindu.com/elections/delhi-assembly/\n",
            "[+] Scanning: https://www.thehindu.com/elections/bihar-assembly/\n",
            "[+] Found 54 total relevant articles from The Hindu\n",
            "🗂️  BJP’s USP in Delhi: Micro-level outreach programmes, door-to-door campaigns\n",
            "🗂️  BJP urges Delhi L-G to restore ‘Sheesh Mahal’ to original state\n",
            "🗂️  L-G’s actions, excise probe, AAP-da led to AAP’s election setback\n",
            "🗂️  We failed to get anti-BJP votes of Dalits, minorities: Delhi Congress chief\n",
            "🗂️  Bihar\n",
            "🗂️  Delhi\n",
            "🗂️  Elections\n",
            "🗂️  Delhi Assembly\n",
            "🗂️  Days after Delhi defeat, Kejriwal to meet CM Mann, Punjab AAP MLAs on February 11\n",
            "🗂️  BJP begins talks to select Delhi CM; oath-taking likely after PM’s U.S. visit\n",
            "🗂️  AAP’s defeat in Delhi election lifts the Congress’s spirits in Punjab\n",
            "🗂️  Watch: Delhi Assembly Elections 2025 Round Up | Major upset for AAP as BJP seals victory\n",
            "🗂️  Delhi election result: Kejriwal, Atishi meet newly elected AAP MLAs, ask them to work for people\n",
            "🗂️  Delhi election results: Fewer MLAs with criminal cases; average asset value surges\n",
            "🗂️  Rekha Gupta takes charge as ninth Chief Minister of Delhi\n",
            "[✓] Saved 15 articles to thehindu_elections_delhi_bihar.csv\n"
          ]
        }
      ]
    }
  ]
}