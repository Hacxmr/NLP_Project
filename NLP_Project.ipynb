{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbju+bdA6pqjYoox3IU1kN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hacxmr/NLP_Project/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Summarization with Bias Detection in News"
      ],
      "metadata": {
        "id": "uHa5h2qlP1A1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tzXNNcxlPeZp",
        "outputId": "40f57401-b5e6-4625-ba53-ff091c84cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas tqdm selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text = \"Instead of promised 'acche din', days of debt have arrived: Congress's dig at Modi Government\"\n",
        "summary = summarizer(text, max_length=20, min_length=5, do_sample=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPEOH6dHPv2R",
        "outputId": "5e2738d7-5578-4120-c7e9-105862cbff65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mmwukqmQBjh",
        "outputId": "435f529a-8672-4da5-942d-97206803ee7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"Instead of promised 'acche din', days of debt have arrived: Congress's dig\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(text, max_length=30, min_length=5, do_sample=False)\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aumBLiwHTx5h",
        "outputId": "1ab4a1cc-dd16-477e-a172-dbb581c372b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 30, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instead of promised 'acche din', days of debt have arrived: Congress's dig at Modi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis"
      ],
      "metadata": {
        "id": "MyZ2vHlRUVq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "sentiment = sentiment_pipeline(summary[0]['summary_text'])\n",
        "print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbkUVAvmT1Mq",
        "outputId": "2805c31c-b806-4cb3-f9bc-6a45aee86550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9930257797241211}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Explore BART/T5 Models"
      ],
      "metadata": {
        "id": "rVEv40PbUxId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-base\",tokenizer=\"t5-base\")\n",
        "\n",
        "\n",
        "input_text = \"summarize: \"+text\n",
        "summary = summarizer(input_text, max_length=20, min_length=5, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-imv1izNUwUq",
        "outputId": "3b7f5057-3330-4501-c48a-5a3916d11504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instead of promised 'acche din', days of debt have arrived .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection - Scraping"
      ],
      "metadata": {
        "id": "P_Ro8XH6rwvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODFDfn60shs2",
        "outputId": "39e4e638-b3ed-438e-f60f-2cc11dd26d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium pandas transformers beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs-b5eWrzXLE",
        "outputId": "0cff8787-b42e-401e-d9aa-ee9d467444d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import tempfile\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")  # fix for profile conflict\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n"
      ],
      "metadata": {
        "id": "x4X8fRB4z2kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from urllib.parse import urljoin\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP Pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "bias_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://www.indiatoday.in/elections\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "KEYWORDS = [\n",
        "    \"delhi election\", \"delhi polls\", \"aap\", \"kejriwal\", \"delhi mla\",\n",
        "    \"bihar election\", \"bihar polls\", \"tejashwi\", \"nitish\", \"bjp\", \"rjd\", \"jdu\",\n",
        "    \"lok sabha\", \"2024\", \"campaign\", \"manifesto\", \"seat sharing\", \"voting\"\n",
        "]\n",
        "MAX_ARTICLES = 15\n",
        "\n",
        "def is_delhi_election(text):\n",
        "    return any(k in text.lower() for k in KEYWORDS)\n",
        "\n",
        "def get_article_links():\n",
        "    r = requests.get(BASE_URL, headers=HEADERS)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    links = []\n",
        "    for a in soup.find_all(\"a\", href=True, title=True):\n",
        "        href = urljoin(BASE_URL, a[\"href\"])\n",
        "        title = a[\"title\"].strip()\n",
        "        if is_delhi_election(title):\n",
        "            links.append((href, title))\n",
        "    return list(set(links))\n",
        "\n",
        "def extract_article_content(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 30)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def summarize_sentiment_bias(text):\n",
        "    summary = summarizer(text[:1024])[0]['summary_text']\n",
        "    sentiment = sentiment_analyzer(summary)[0]\n",
        "    bias = bias_classifier(summary, candidate_labels=[\"pro-government\", \"anti-government\", \"neutral\"])\n",
        "    return summary, sentiment[\"label\"], sentiment[\"score\"], bias[\"labels\"][0], bias[\"scores\"][0]\n",
        "\n",
        "def main():\n",
        "    articles = get_article_links()\n",
        "    print(f\"[+] Found {len(articles)} Delhi election-related articles\")\n",
        "\n",
        "    data = []\n",
        "    for i, (url, title) in enumerate(articles[:MAX_ARTICLES]):\n",
        "        print(f\"[{i+1}] Scraping: {title}\")\n",
        "        content = extract_article_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, senti_score, bias, bias_score = summarize_sentiment_bias(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"content\": content,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sentiment_score\": round(senti_score, 2),\n",
        "                \"bias\": bias,\n",
        "                \"bias_score\": round(bias_score, 2),\n",
        "                \"source\": \"India Today\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1, 2.5))\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"indiatoday_elections.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(df)} articles to indiatoday_delhi_elections.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aij83w9k9Z4D",
        "outputId": "e39eddfc-c2dc-4a98-8b0c-fcf684a3af65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Found 37 Delhi election-related articles\n",
            "[1] Scraping: BJP's Mohan Singh Bisht proposes to rename Mustafabad to Shivpuri\n",
            "[2] Scraping: Chose wrong path, focused on liquor: Anna Hazare on Arvind Kejriwal's poll loss\n",
            "[3] Scraping: Amanatullah Khan slams Rahul Gandhi for campaigning in Okhla to 'defeat' AAP\n",
            "[4] Scraping: BJP top brass holds hectic parleys to finalise Delhi's next Chief Minister\n",
            "[5] Scraping: Down but not out: Is Nitish Kumar still the X-factor in Bihar Politics?\n",
            "[6] Scraping: Nitish Kumar changes party for kursi: Congress targets BJP-JDU alliance\n",
            "[7] Scraping: BJP seeks dominance in Delhi civic body following Assembly success\n",
            "[8] Scraping: RJD leader pitches Tejashwi Yadav as chief ministerial face ahead of meet today\n",
            "[9] Scraping: Is Kanhaiya Kumar stealing Tejashwi Yadav's thunder on jobs, migration in Bihar?\n",
            "[10] Scraping: BJP considers candidates for Delhi Chief Minister's post after win\n",
            "[11] Scraping: BJP to release multiple CAG reports in upcoming Delhi assembly session\n",
            "[12] Scraping: Chirag Paswan's Bihar bid: Pressure tactic on BJP or strategy to replace Nitish?\n",
            "[13] Scraping: LJP (RV) projects Chirag Paswan as Chief Minister face, challenging Nitish Kumar\n",
            "[14] Scraping: Delhi BJP chief seeks to meet Lt Governor amid speculations over chief minister\n",
            "[15] Scraping: Masterstroke or damage-control? BJP's caste census sets ball rolling in Bihar\n",
            "[✓] Saved 15 articles to indiatoday_delhi_elections.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NDTV - SCRAPING AND ANALYSIS"
      ],
      "metadata": {
        "id": "iALI9BDjJsfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import tempfile\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP Pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "bias_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Focus keywords for Delhi and Bihar elections\n",
        "KEYWORDS = [\n",
        "    \"delhi election\", \"delhi polls\", \"aap\", \"kejriwal\", \"delhi mla\",\n",
        "    \"bihar election\", \"bihar polls\", \"tejashwi\", \"nitish\", \"bjp\", \"rjd\", \"jdu\",\n",
        "    \"lok sabha\", \"2024\", \"campaign\", \"manifesto\", \"seat sharing\", \"voting\"\n",
        "]\n",
        "\n",
        "# Selenium Setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "def is_target_region(text):\n",
        "    return any(keyword in text.lower() for keyword in KEYWORDS)\n",
        "\n",
        "def get_articles():\n",
        "    url = \"https://www.ndtv.com/elections\"\n",
        "    print(f\"[+] Visiting: {url}\")\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "    links = []\n",
        "\n",
        "    for tag in soup.find_all(\"a\", class_=\"crd_lnk\", href=True):\n",
        "        title = tag.get_text(strip=True)\n",
        "        href = tag[\"href\"]\n",
        "        if title and is_target_region(title):\n",
        "            links.append((title, href))\n",
        "\n",
        "    return list(set(links))\n",
        "\n",
        "def extract_article_content(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 30)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def analyze_text(text):\n",
        "    try:\n",
        "        summary = summarizer(text[:1024])[0][\"summary_text\"]\n",
        "        sentiment_result = sentiment_analyzer(summary)[0]\n",
        "        sentiment = sentiment_result[\"label\"]\n",
        "        sentiment_score = round(sentiment_result[\"score\"], 2)\n",
        "\n",
        "        bias_result = bias_classifier(summary, candidate_labels=[\"pro-government\", \"anti-government\", \"neutral\"])\n",
        "        bias = bias_result[\"labels\"][0]\n",
        "        bias_score = round(bias_result[\"scores\"][0], 2)\n",
        "\n",
        "        return summary, sentiment, sentiment_score, bias, bias_score\n",
        "    except Exception as e:\n",
        "        print(f\"[-] NLP error: {e}\")\n",
        "        return \"Summary failed\", \"UNKNOWN\", 0.0, \"UNKNOWN\", 0.0\n",
        "\n",
        "def main():\n",
        "    articles = get_articles()\n",
        "    print(f\"[+] Found {len(articles)} Delhi/Bihar election-related articles\")\n",
        "\n",
        "    data = []\n",
        "    for i, (title, url) in enumerate(articles[:15]):\n",
        "        print(f\"[{i+1}] Scraping: {title}\")\n",
        "        content = extract_article_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, senti_score, bias, bias_score = analyze_text(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"content\": content,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sentiment_score\": senti_score,\n",
        "                \"bias\": bias,\n",
        "                \"bias_score\": bias_score,\n",
        "                \"source\": \"NDTV\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"ndtv_delhi_bihar_election.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(df)} articles to ndtv_delhi_bihar_election.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz2dM9Jz-Tqd",
        "outputId": "4f85becd-ca73-492a-c7e9-0f009e408377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Visiting: https://www.ndtv.com/elections\n",
            "[+] Found 15 Delhi/Bihar election-related articles\n",
            "[1] Scraping: \"Governance Is Not Theatrics\": PM's Dig At AAP After BJP Wins Delhi\n",
            "[2] Scraping: Delhi Oath Ceremony On Thursday, BJP Yet To Name Chief Minister\n",
            "[3] Scraping: \"AAP Was Finished When...\": Ex Leader Ashutosh's Post After Delhi Drubbing\n",
            "[4] Scraping: Video: AAP's Office Doors Shut As Arvind Kejriwal, Manish Sisodia Lose Delhi\n",
            "[5] Scraping: \"Was Doing Good Job, But...\": Anna Hazare On Arvind Kejriwal's Poll Defeat\n",
            "[6] Scraping: \"2026MeinBengalKi Baari\": Mamata Banerjee Warned After BJP's Delhi Win\n",
            "[7] Scraping: AAP's Saurabh Bharadwaj Turns YouTuber After Delhi Election Loss\n",
            "[8] Scraping: \"Rule Of Lies Has Ended\": Amit Shah On BJP's Delhi Victory\n",
            "[9] Scraping: AAP To Replace Chief Minister In Punjab? What Bhagwant Mann Said\n",
            "[10] Scraping: How AAP-Congress Feud Split Opposition Votes\n",
            "[11] Scraping: AAP vs Congress In Punjab Over '30 MLAs In Touch With Us' Claim\n",
            "[12] Scraping: Congress-AAP's Unity Strongly Needed, Should Have Fought Delhi Polls Together: Amartya Sen\n",
            "[13] Scraping: Delhi Election Results 2025: BJP's Comeback, AAP's Loss Spark A Meme Fest\n",
            "[14] Scraping: \"Give Him Some Water\": PM Modi Pauses Speech To Check On Unwell BJP Worker\n",
            "[15] Scraping: PM Modi's Big Mention Of Yamuna In Delhi Victory Speech To BJP Workers\n",
            "[✓] Saved 15 articles to ndtv_delhi_bihar_election.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import tempfile\n",
        "from transformers import pipeline\n",
        "import signal\n",
        "import sys\n",
        "\n",
        "# Load NLP models\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Bias classifier\n",
        "def classify_bias(text):\n",
        "    text = text.lower()\n",
        "    pro_gov = any(word in text for word in [\"modi\", \"bjp\", \"nda\", \"development\"])\n",
        "    anti_gov = any(word in text for word in [\"scam\", \"failure\", \"protest\", \"resign\", \"criticism\", \"controversy\"])\n",
        "    if pro_gov and not anti_gov:\n",
        "        return \"PRO-GOVERNMENT\"\n",
        "    elif anti_gov and not pro_gov:\n",
        "        return \"ANTI-GOVERNMENT\"\n",
        "    elif pro_gov and anti_gov:\n",
        "        return \"MIXED\"\n",
        "    return \"NEUTRAL\"\n",
        "\n",
        "# Keywords to filter articles\n",
        "KEYWORDS = [\"delhi election\", \"bihar election\", \"2024\", \"lok sabha\", \"nda\", \"india bloc\", \"upa\", \"bjp\", \"congress\"]\n",
        "\n",
        "# Setup Selenium\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# Ensure driver quits on script interruption\n",
        "def quit_driver(*args):\n",
        "    driver.quit()\n",
        "    sys.exit(0)\n",
        "\n",
        "signal.signal(signal.SIGINT, quit_driver)\n",
        "signal.signal(signal.SIGTERM, quit_driver)\n",
        "\n",
        "# Step 1: Collect article links from TOI election pages\n",
        "def get_toi_articles():\n",
        "    base_urls = [\n",
        "        \"https://timesofindia.indiatimes.com/elections/assembly-elections/delhi\",\n",
        "        \"https://timesofindia.indiatimes.com/elections/assembly-elections/bihar\"\n",
        "    ]\n",
        "    articles = []\n",
        "\n",
        "    for url in base_urls:\n",
        "        driver.get(url)\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        links = soup.find_all(\"a\", href=True)\n",
        "        for tag in links:\n",
        "            title = tag.get_text(strip=True)\n",
        "            link = tag[\"href\"]\n",
        "            if title and any(k in title.lower() for k in KEYWORDS):\n",
        "                if not link.startswith(\"http\"):\n",
        "                    link = \"https://timesofindia.indiatimes.com\" + link\n",
        "                articles.append((title, link))\n",
        "            if len(articles) >= 15:\n",
        "                return articles\n",
        "    return articles\n",
        "\n",
        "# Step 2: Extract article content\n",
        "def extract_article(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        article_blocks = soup.find_all(\"div\", class_=\"Normal\")\n",
        "        if not article_blocks:\n",
        "            article_blocks = soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in article_blocks if len(p.text.strip()) > 30)\n",
        "        return content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Error extracting article from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 3: Main pipeline\n",
        "def main():\n",
        "    print(\"[+] Scraping TOI for Delhi & Bihar election news...\")\n",
        "    articles = get_toi_articles()\n",
        "    print(f\"[+] Found {len(articles)} relevant articles\")\n",
        "\n",
        "    records = []\n",
        "    for i, (title, url) in enumerate(articles):\n",
        "        print(f\"[{i+1}] {title}\")\n",
        "        content = extract_article(url)\n",
        "        if content:\n",
        "            try:\n",
        "                trimmed_content = content[:1024 * 4]  # Conservative length estimate for token limit\n",
        "                summary = summarizer(trimmed_content)[0]['summary_text']\n",
        "                sentiment_result = sentiment_analyzer(summary)[0]\n",
        "                sentiment = sentiment_result['label']\n",
        "                score = round(sentiment_result['score'], 2)\n",
        "                bias = classify_bias(summary)\n",
        "\n",
        "                records.append({\n",
        "                    \"title\": title,\n",
        "                    \"url\": url,\n",
        "                    \"summary\": summary,\n",
        "                    \"sentiment\": sentiment,\n",
        "                    \"confidence\": score,\n",
        "                    \"bias\": bias,\n",
        "                    \"content\": content,\n",
        "                    \"source\": \"TOI\"\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[-] Failed NLP processing for: {url} | Error: {e}\")\n",
        "        else:\n",
        "            print(f\"[-] Skipping empty content: {url}\")\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "\n",
        "    # Save data\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(\"toi_delhi_bihar_elections.csv\", index=False)\n",
        "    print(\"[✓] Data saved to toi_delhi_bihar_elections.csv\")\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    finally:\n",
        "        driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGaNDakI-Sw",
        "outputId": "1f8ee900-c745-47bc-c50d-8bd6368e1070"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scraping TOI for Delhi & Bihar election news...\n",
            "[+] Found 15 relevant articles\n",
            "[1] Delhi Elections\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 130. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=65)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2] Lok Sabha 2024\n",
            "[3] DELHI ELECTIONS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 130. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=65)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4] Congress Candidates List\n",
            "[5] BJP Candidate List\n",
            "[6] Delhi Election 2025\n",
            "[7] NewsFeb 9, 2025What win in Delhi polls for 11 councillors means for MCDOf 22 councillors in the Delhi assembly elections, 11 won with BJP securing eight seats and AAP three. BJP now has 112 councillors and AAP 119. The MCD has 12 vacant positions, impacting mayoral election dynamics. BJP may gain an advantage due to additional support from MPs and MLAs, despite past tight race margins.\n",
            "[-] Skipping empty content: https://timesofindia.indiatimes.com/elections/assembly-elections/delhi/news/what-win-for-11-councillors-means-for-mcd/articleshow/118102373.cms\n",
            "[8] NewsFeb 9, 2025'All dreams of my father will be fulfilled': BJP's Parvesh Verma pays tribute to former Delhi CM Sahib Singh Verma\n",
            "[9] NewsFeb 9, 2025After big Delhi win, BJP's Parvesh Verma pays tribute to father, vows to fulfil his ‘unfinished works’\n",
            "[10] NewsFeb 9, 2025'Shameless': Rajya Sabha MP Swati Maliwal on Atishi's dance video after her poll win, AAP's defeatRajya Sabha MP Swati Maliwal criticized AAP leader Atishi for celebrating her win despite the party’s significant defeat in the Delhi assembly elections. Atishi, having won the Kalkaji seat, expressed gratitude while acknowledging the party's overall loss and stated their ongoing battle against BJP, who secured 48 seats.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11] 'Memories of 1993 have refreshed': BJP's Parvesh Verma who took New Delhi from AAP's Arvind Kejriwal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12] Delhi election results 2025: Winners and losers\n",
            "[13] Delhi Elections\n",
            "[14] Delhi Election Result 2025\n",
            "[15] Delhi Elections\n",
            "[✓] Data saved to toi_delhi_bihar_elections.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium pandas beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkVdt0tqGm1c",
        "outputId": "4016c22e-59bb-43ff-a127-3eb04543b7f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Bias detection (basic rule-based)\n",
        "def detect_bias(text):\n",
        "    t = text.lower()\n",
        "    if any(k in t for k in [\"bjp\", \"modi\", \"development\"]):\n",
        "        return \"Pro-Government\"\n",
        "    if any(k in t for k in [\"aam aadmi\", \"congress\", \"rjd\", \"opposition\"]):\n",
        "        return \"Anti-Government\"\n",
        "    return \"Neutral\"\n",
        "\n",
        "# Keywords for relevance\n",
        "KEYWORDS = [\"delhi\", \"bihar\", \"election\", \"assembly\", \"mla\", \"poll\", \"campaign\"]\n",
        "\n",
        "# Multiple sources from The Hindu\n",
        "START_URLS = [\n",
        "    \"https://www.thehindu.com/elections/\",\n",
        "    \"https://www.thehindu.com/elections/delhi-assembly/\",\n",
        "    \"https://www.thehindu.com/elections/bihar-assembly/\"\n",
        "]\n",
        "\n",
        "def is_relevant(text):\n",
        "    return any(k in text.lower() for k in KEYWORDS)\n",
        "\n",
        "def fetch_article_links():\n",
        "    all_links = []\n",
        "    seen = set()\n",
        "    for url in START_URLS:\n",
        "        print(f\"[+] Scanning: {url}\")\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        for div in soup.select(\"div.right-content\"):\n",
        "            h3 = div.find(\"h3\", class_=\"title\")\n",
        "            if h3 and h3.a:\n",
        "                title = h3.a.get_text(strip=True)\n",
        "                href = h3.a[\"href\"]\n",
        "                if href not in seen and is_relevant(title):\n",
        "                    all_links.append((title, href))\n",
        "                    seen.add(href)\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            title = a.get_text(strip=True)\n",
        "            href = a[\"href\"]\n",
        "            if href.startswith(\"https://www.thehindu.com\") and href not in seen and is_relevant(title):\n",
        "                all_links.append((title, href))\n",
        "                seen.add(href)\n",
        "    return all_links\n",
        "\n",
        "def extract_content(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paras = soup.select(\"div.ece-1705723324092 p\") or soup.find_all(\"p\")\n",
        "        return \" \".join(p.get_text(strip=True) for p in paras if len(p.get_text(strip=True)) > 30)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def analyze(text):\n",
        "    try:\n",
        "        summary = summarizer(text[:1024])[0]['summary_text']\n",
        "        sent = sentiment_analyzer(summary)[0]\n",
        "        bias = detect_bias(summary)\n",
        "        return summary, sent['label'], round(sent['score'], 2), bias\n",
        "    except:\n",
        "        return \"Summary failed\", \"UNKNOWN\", 0.0, \"Neutral\"\n",
        "\n",
        "def main():\n",
        "    articles = fetch_article_links()\n",
        "    print(f\"[+] Found {len(articles)} total relevant articles from The Hindu\")\n",
        "    data = []\n",
        "    for title, url in articles[:15]:\n",
        "        print(f\"🗂️  {title}\")\n",
        "        content = extract_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, sent_score, bias = analyze(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sent_score\": sent_score,\n",
        "                \"bias\": bias,\n",
        "                \"source\": \"The Hindu\"\n",
        "            })\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"thehindu_elections_delhi_bihar.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(data)} articles to thehindu_elections_delhi_bihar.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILIza-HGG84c",
        "outputId": "8b3d4b06-26d5-41f3-cc76-25951e79afca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scanning: https://www.thehindu.com/elections/\n",
            "[+] Scanning: https://www.thehindu.com/elections/delhi-assembly/\n",
            "[+] Scanning: https://www.thehindu.com/elections/bihar-assembly/\n",
            "[+] Found 54 total relevant articles from The Hindu\n",
            "🗂️  BJP’s USP in Delhi: Micro-level outreach programmes, door-to-door campaigns\n",
            "🗂️  BJP urges Delhi L-G to restore ‘Sheesh Mahal’ to original state\n",
            "🗂️  L-G’s actions, excise probe, AAP-da led to AAP’s election setback\n",
            "🗂️  We failed to get anti-BJP votes of Dalits, minorities: Delhi Congress chief\n",
            "🗂️  Bihar\n",
            "🗂️  Delhi\n",
            "🗂️  Elections\n",
            "🗂️  Delhi Assembly\n",
            "🗂️  Days after Delhi defeat, Kejriwal to meet CM Mann, Punjab AAP MLAs on February 11\n",
            "🗂️  BJP begins talks to select Delhi CM; oath-taking likely after PM’s U.S. visit\n",
            "🗂️  AAP’s defeat in Delhi election lifts the Congress’s spirits in Punjab\n",
            "🗂️  Watch: Delhi Assembly Elections 2025 Round Up | Major upset for AAP as BJP seals victory\n",
            "🗂️  Delhi election result: Kejriwal, Atishi meet newly elected AAP MLAs, ask them to work for people\n",
            "🗂️  Delhi election results: Fewer MLAs with criminal cases; average asset value surges\n",
            "🗂️  Rekha Gupta takes charge as ninth Chief Minister of Delhi\n",
            "[✓] Saved 15 articles to thehindu_elections_delhi_bihar.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from transformers import pipeline\n",
        "\n",
        "# NLP pipelines\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Bias detection (rule-based)\n",
        "def detect_bias(text):\n",
        "    t = text.lower()\n",
        "    if any(k in t for k in [\"bjp\", \"modi\", \"development\"]):\n",
        "        return \"Pro-Government\"\n",
        "    if any(k in t for k in [\"aam aadmi\", \"congress\", \"rjd\", \"opposition\"]):\n",
        "        return \"Anti-Government\"\n",
        "    return \"Neutral\"\n",
        "\n",
        "# Relevant keywords\n",
        "KEYWORDS = [\"delhi\", \"bihar\", \"election\", \"assembly\", \"mla\", \"poll\", \"campaign\"]\n",
        "\n",
        "# URLs to scan\n",
        "START_URLS = [\n",
        "    \"https://www.thehindu.com/elections/\",\n",
        "    \"https://www.thehindu.com/elections/delhi-assembly/\",\n",
        "    \"https://www.thehindu.com/elections/bihar-assembly/\"\n",
        "]\n",
        "\n",
        "def is_relevant(text):\n",
        "    return any(k in text.lower() for k in KEYWORDS)\n",
        "\n",
        "# Step 1: Collect article links\n",
        "def fetch_article_links():\n",
        "    all_links = []\n",
        "    seen = set()\n",
        "    for url in START_URLS:\n",
        "        print(f\"[+] Scanning: {url}\")\n",
        "        try:\n",
        "            r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "            # Structured blocks\n",
        "            for div in soup.select(\"div.right-content\"):\n",
        "                h3 = div.find(\"h3\", class_=\"title\")\n",
        "                if h3 and h3.a:\n",
        "                    title = h3.a.get_text(strip=True)\n",
        "                    href = h3.a[\"href\"]\n",
        "                    if href not in seen and is_relevant(title):\n",
        "                        all_links.append((title, href))\n",
        "                        seen.add(href)\n",
        "\n",
        "            # Fallback: general <a> links\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                title = a.get_text(strip=True)\n",
        "                href = a[\"href\"]\n",
        "                if href.startswith(\"https://www.thehindu.com\") and href not in seen and is_relevant(title):\n",
        "                    all_links.append((title, href))\n",
        "                    seen.add(href)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[-] Error accessing {url}: {e}\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "# Step 2: Extract full article content\n",
        "def extract_content(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paras = soup.select(\"div.ece-1705723324092 p\") or soup.find_all(\"p\")\n",
        "        content = \" \".join(p.get_text(strip=True) for p in paras if len(p.get_text(strip=True)) > 30)\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"[-] Failed to extract {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 3: Summarization + Sentiment + Bias\n",
        "def analyze(text):\n",
        "    try:\n",
        "        truncated = text[:1024 * 4]  # Prevent token overflow\n",
        "        summary = summarizer(truncated)[0]['summary_text']\n",
        "        sent = sentiment_analyzer(summary)[0]\n",
        "        bias = detect_bias(summary)\n",
        "        return summary, sent['label'], round(sent['score'], 2), bias\n",
        "    except Exception as e:\n",
        "        print(f\"[-] NLP error: {e}\")\n",
        "        return \"Summary failed\", \"UNKNOWN\", 0.0, \"Neutral\"\n",
        "\n",
        "# Step 4: Main collection function\n",
        "def main():\n",
        "    articles = fetch_article_links()\n",
        "    print(f\"[+] Found {len(articles)} total relevant articles from The Hindu\")\n",
        "\n",
        "    data = []\n",
        "    for i, (title, url) in enumerate(articles[:55]):\n",
        "        print(f\"[{i+1}] 🗂️  {title}\")\n",
        "        content = extract_content(url)\n",
        "        if content:\n",
        "            summary, sentiment, sent_score, bias = analyze(content)\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"sent_score\": sent_score,\n",
        "                \"bias\": bias,\n",
        "                \"source\": \"The Hindu\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[-] Skipped due to empty content.\")\n",
        "        time.sleep(random.uniform(1.5, 2.5))  # polite scraping\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"thehindu_elections_delhi_bihar.csv\", index=False)\n",
        "    print(f\"[✓] Saved {len(data)} articles to thehindu_elections_delhi_bihar.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KUPDOkjW7dV",
        "outputId": "8a7b63e1-7081-41c1-f061-87f60ca3974e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scanning: https://www.thehindu.com/elections/\n",
            "[+] Scanning: https://www.thehindu.com/elections/delhi-assembly/\n",
            "[+] Scanning: https://www.thehindu.com/elections/bihar-assembly/\n",
            "[+] Found 54 total relevant articles from The Hindu\n",
            "[1] 🗂️  BJP’s USP in Delhi: Micro-level outreach programmes, door-to-door campaigns\n",
            "[2] 🗂️  BJP urges Delhi L-G to restore ‘Sheesh Mahal’ to original state\n",
            "[3] 🗂️  L-G’s actions, excise probe, AAP-da led to AAP’s election setback\n",
            "[4] 🗂️  We failed to get anti-BJP votes of Dalits, minorities: Delhi Congress chief\n",
            "[5] 🗂️  Bihar\n",
            "[6] 🗂️  Delhi\n",
            "[7] 🗂️  Elections\n",
            "[8] 🗂️  Delhi Assembly\n",
            "[9] 🗂️  Days after Delhi defeat, Kejriwal to meet CM Mann, Punjab AAP MLAs on February 11\n",
            "[10] 🗂️  BJP begins talks to select Delhi CM; oath-taking likely after PM’s U.S. visit\n",
            "[11] 🗂️  AAP’s defeat in Delhi election lifts the Congress’s spirits in Punjab\n",
            "[12] 🗂️  Watch: Delhi Assembly Elections 2025 Round Up | Major upset for AAP as BJP seals victory\n",
            "[13] 🗂️  Delhi election result: Kejriwal, Atishi meet newly elected AAP MLAs, ask them to work for people\n",
            "[14] 🗂️  Delhi election results: Fewer MLAs with criminal cases; average asset value surges\n",
            "[15] 🗂️  Rekha Gupta takes charge as ninth Chief Minister of Delhi\n",
            "[16] 🗂️  BJP MLA Vijender Gupta elected as Speaker of Delhi Assembly\n",
            "[17] 🗂️  I am very disciplined worker of BJP; will fulfill responsibility given to me: New Delhi MLA Parvesh Verma\n",
            "[18] 🗂️  Delhi Cabinet decides to implement Ayushman Bharat, table CAG reports\n",
            "[19] 🗂️  Full list of Delhi Ministers\n",
            "[20] 🗂️  Women in Delhi to get ₹2,500 monthly aid by March 8: CM-designate Rekha Gupta\n",
            "[21] 🗂️  Rohini MLA Vijender Gupta is BJP's candidate to be new Delhi Assembly speaker\n",
            "[22] 🗂️  FeaturedMaharashtra Assembly elections 2024 | MIT-SOG-Lokniti-CSDS pre-poll surveyMaharashtra AssemblyThe Hindu BureauREAD NOW\n",
            "[23] 🗂️  Congress, AAP didn’t address issues of people of Delhi, we did so on day one: Rekha Gupta\n",
            "[24] 🗂️  Delhi oath-taking ceremony: Parvesh Verma, Kapil Mishra to be part of Delhi Cabinet, six MLAs to join as Ministers\n",
            "[25] 🗂️  Delhi CM oath-taking ceremony highlights: Delhi Cabinet decides to implement Ayushman Bharat, table 14 pending CAG reports\n",
            "[26] 🗂️  “I will stand up to PM Modi’s expectations”: Delhi CM-designate Rekha Gupta\n",
            "[27] 🗂️  Delhi new CM announcement highlights: CM-designate Rekha stakes claim to form govt, LG accepts\n",
            "[28] 🗂️  BJP leaders inspect Ramlila Ground before 'historic, grand' oath-taking ceremony for Delhi CM\n",
            "[29] 🗂️  BJP government in Delhi likely to be sworn in next week\n",
            "[30] 🗂️  Infighting within BJP preventing selection of Delhi CM, says AAP\n",
            "[31] 🗂️  Congress-AAP should have fought Delhi polls together, their unity strongly needed: Amartya Sen\n",
            "[32] 🗂️  BJP poll committees meet to discuss Delhi election result\n",
            "[33] 🗂️  Delhi HC asked retired judge to approach SC on plea against “freebies” in elections\n",
            "[34] 🗂️  Watch: Delhi Assembly Elections 2025 | Key winners and losers\n",
            "[35] 🗂️  Make Tejashwi Yadav CM of Bihar at any cost, Lalu Prasad appeals to people\n",
            "[36] 🗂️  Tejashwi promises 200 units of free electricity if the mahagathbandhan returns to power in Bihar next year\n",
            "[37] 🗂️  Bihar by-polls: RJD names candidates for 3 seats, one to be contested by CPI(ML)\n",
            "[38] 🗂️  Bihar’s Tarari Assembly bypoll: Prashant Kishor’s party fields ex-Vice Chief of Army Staff, S.K. Singh\n",
            "[39] 🗂️  Bihar Assembly bypoll: Independent candidate Shankar Singh defeats RJD and JD(U)\n",
            "[40] 🗂️  No mismatch in EVM, VVPAT counts in Bihar polls: Election Commission\n",
            "[41] 🗂️  NDA, Mahagathbandhan field candidates for Bihar Speaker’s post\n",
            "[42] 🗂️  Jitan Ram Manjhi sworn in as pro-tem Speaker of Bihar assembly\n",
            "[43] 🗂️  8 Bihar Ministers facing criminal cases, says Association for Democratic Reforms\n",
            "[44] 🗂️  First Cabinet meet of new Bihar govt. approves 5-day legislature session\n",
            "[45] 🗂️  RJD, Congress to boycott swearing-in ceremony of new govt. in Bihar\n",
            "[46] 🗂️  Coronavirus: Nearly 160 tonnes of biomedical waste generated during Bihar polls\n",
            "[47] 🗂️  Nitish Kumar to take oath as Bihar CM on November 16\n",
            "[48] 🗂️  Bihar Assembly elections | NDA to meet on Nov.15 to elect leader\n",
            "[49] 🗂️  Ours was a model electoral campaign: RJD’s Manoj K Jha\n",
            "[50] 🗂️  Bihar Assembly elections | ‘Only BJP can look into’ role of LJP, says Nitish Kumar\n",
            "[51] 🗂️  Bihar Assembly election | Grand Alliance lost because of Congress’ poor performance, says Tariq Anwar\n",
            "[52] 🗂️  Bihar Assembly election results | On irregularities in counting of votes, CEC says ultimate decision lies with people\n",
            "[53] 🗂️  Bihar Assembly elections | ‘Nitish Kumar’s model of development — ending mafia raj, jungle raj — has found popularity among people’\n",
            "[54] 🗂️  EC to go for 100% webcasting of polling stations for closer monitoring of voting procedure\n",
            "[✓] Saved 54 articles to thehindu_elections_delhi_bihar.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merging the files\n"
      ],
      "metadata": {
        "id": "Mqdqp3B2yGuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load individual files\n",
        "files = {\n",
        "    \"TOI\": \"/content/toi_delhi_bihar_elections.csv\",\n",
        "    \"NDTV\": \"/content/ndtv_delhi_bihar_election.csv\",\n",
        "    \"IndiaToday\": \"/content/indiatoday_elections.csv\",\n",
        "    \"TheHindu\": \"/content/thehindu_elections_delhi_bihar.csv\"\n",
        "}\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "for source, path in files.items():\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Normalize column names (some files may differ slightly)\n",
        "    df.rename(columns={\n",
        "        \"sentiment_score\": \"confidence\",\n",
        "        \"sent_score\": \"confidence\",\n",
        "        \"bias_score\": \"bias_confidence\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Ensure all expected columns are present\n",
        "    for col in [\"title\", \"url\", \"summary\", \"sentiment\", \"confidence\", \"bias\", \"content\", \"source\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None  # Add missing columns with default None\n",
        "\n",
        "    df[\"source\"] = source  # Add consistent source label\n",
        "    dataframes.append(df[[\"title\", \"url\", \"summary\", \"sentiment\", \"confidence\", \"bias\", \"content\", \"source\"]])\n",
        "\n",
        "# Concatenate all into one DataFrame\n",
        "merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save to new file\n",
        "merged_df.to_csv(\"merged_election_news_dataset.csv\", index=False)\n",
        "\n",
        "print(f\"[✓] Merged {len(merged_df)} total articles into 'merged_election_news_dataset.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmpWpAnfyJv5",
        "outputId": "d39058cc-a84f-437b-ca03-2b146f00f89c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✓] Merged 98 total articles into 'merged_election_news_dataset.csv'\n"
          ]
        }
      ]
    }
  ]
}